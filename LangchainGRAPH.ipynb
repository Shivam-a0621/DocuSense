{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-community langchain-openai neo4j\n",
    "# %pip install openllm\n",
    "# %pip install -qU langchain-groq\n",
    "# %pip install -U langchain-huggingface\n",
    "# %pip install faiss-cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1path= \"books\\Global Warming.pdf\"\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "reader = PdfReader(book1path)\n",
    "page = reader.pages[0]\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "text=extract_text_from_pdf(book1path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"uploads\\globalwarming.txt\"\n",
    "with open(path, \"r\", encoding=\"UTF-8\") as file:\n",
    "    loaded_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split= CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "splitted_text= text_split.split_text(loaded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings= HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings= HuggingFaceEmbeddings()\n",
    "vectorstore= FAISS.from_texts(splitted_text, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"vector_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(\"vector_index\", embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='its impact on the construction of this buil ding whose roof we call \\x93“emission cuts\". \\nThe calculation process at a corporate level faces the following obstacles:  \\nx Great difficulty reaching scope 3. Collectin g the supplier\\x92’s indirect footprint is an \\nimpossible mission for many corporations. In  a d d i t i o n  t o  t h e  p r o c e d u r a l  d i f f i c u l t y  \\ninvolved in \"forcing\" providers to do the calculation, it is based on a totally non-\\nstandard assembly process in which each pr ovider chooses the method to calculate the \\nfootprint of their products. This creates great distortion and the results lack credibility.  \\nx Voluntary choice of the calculation method, and the scope and the emission factors as \\nlong as they come from \\'reliable source s\\'. This leaves the spreadsheet open.  \\nx Inconsistency with the footprint of products  or services when these are calculated.  \\nx Legislation compliance (CO 2 e m i s s i o n  r i g h t s )  r a t h e r  t h a n  s e a r c h i n g  f o r  s c e n a r i o s  o f  \\ncompetitiveness among enterprises. \\nx Risk of outsourcing scope 3. In deed, if it is decided not to calculate the footprint, then \\nall that is needed is to outsource the activiti es (eg. transportation) so as not to include \\nthe footprint in the studies as they  are not part of scopes 1 and 2.  \\nx Risk of dispersion of the network.  This is perhaps the most serious drawback. The \\ncorporate carbon footprint, despite all the potential it has to do a complete analysis \\nof the corporation\\x92’s resour ce consumption, may become a mere bureaucratic \\nprocedure.  \\nx It is not possible to compare emission intensity. The basic indicator that informs us \\nabout CO 2 emissions per monetary income of a corporation is disabled by not including \\nall ranges in the calculation studies.  \\nRegarding the approach to products based on a life cycle analysis, the following are \\nidentified:'),\n",
       " Document(page_content='climate change. Indeed, there is nothing more useful for companies than to provide \\ninformation that facilitates the reduction of emissions in relative terms (per unit of \\nproduction of a product or serv ice, and emissive intensity), but also in absolute terms (for \\nthe whole corporation). It\\x92’s of little use if we lower emissions relatively while, on the other \\nhand, corporate emissions grow due to other ac tions that are not within the scope of the \\ncurrent study. \\nThe roof \\nImagine that what we want to do is build a house with a roof called \"to lower emissions\" \\nand the aim is that the roof shou ld be as large as possible, so that the larger the size of the \\nroof, the greater our success in fighting climate change. \\nBut we cannot put a roof over nothing; we n eed a structure that supports it. What are the \\nrequirements, given that the greater the support , the greater the roof \"to lower emissions\\x94” \\nwill be? \\nThe beams \\nTo lower carbon emissions there are few roads to choose from. A simple but subtly \\ndevastating vision of the problem indicate s that we can do basically three things: \\nx Change our patterns of production, either by identifying proces ses for improvement, \\nidentifying good product design that is more  environmentally friendly in the vector of \\nclimate change. \\nx Identify measures of eco-efficiency in th e consumption of energy and materials in our \\nbusiness and production processes. \\nx Change our habits, both from the standpoi nt of providing information to the final \\nconsumer (B2C) of our products and se rvices, and to provide ourselves with \\ninformation from our network of suppliers (B2B) in order to help us to inherit the \\nsmallest footprint possible of products and services that feed our production system. \\nThese are the three basic and essential beams re quired to sustain our roof, and if they are \\nwell-managed they will allow us to reduce emissions from our corporate activities.'),\n",
       " Document(page_content='These are the three basic and essential beams re quired to sustain our roof, and if they are \\nwell-managed they will allow us to reduce emissions from our corporate activities. \\nNote that all three require the processing of data quickly and reliably. Let\\x92’s explore this \\npoint that will lead us to the following levels of support for our house. \\nThe columns \\nHow can we change our production and consumption patterns, and at the same time \\nidentify eco-efficient measures in our activities to help  us cut emissions?  \\nA New Perspective for Labeling the Carb on Footprint Against Climate Change 9 \\nThere is a clear answer to this  question ... we must have reliable and quality data, and this \\ninformation must have three properties that ensure the stability of our house: \\nx Accessibility \\nx Transparency \\nx Comparability \\nLet\\x92’s consider each of these three points together because all three are intimately \\nintertwined. \\nWe refer to accessibility  as the option for all businesses, from the smallest company to the \\nlarge corporations, to make a claim for a carb on footprint certificate for their products, \\naccording to the prices and time frames of the projects adapted to the size of the contracting \\ncompany, without omitting, in any case, the other two pillars: transparency and \\ncomparability. The incorporation process of calc ulating all the business is necessary; a global \\nproblem requires the involvement of all. \\nAt present, the size of projects based on calc ulation techniques using the classic life cycle \\nanalysis, in which a link in the chain (the co mpany who wants to calculate the footprint of \\nits product) bears the burden of the whole calculation effort by drawing up complete \\nprocess maps for the product and its life cycle.  However, owing to the sheer size of these \\nprojects, both financially and operationally, they  cannot be assumed by the majority of small \\nand medium-sized enterprises.'),\n",
       " Document(page_content='retain heat from the earth in the form of long wavelength radiation, which leads to the \\ngreenhouse effect.  \\nReports issued by the Intergov ernmental Panel on Climate Ch ange (IPCC), which includes \\nthe largest community of experts, are warning us that, like everything in life, a little bit of \\neverything is good but too much of one thing can be lethal. \\nOne of the main problems is the extraordin arily high rate of GHG emissions which our \\nsociety has been generating for more than 100 years. This inhibits any reaction from the \\nflora and fauna as well as the human race , which is encountering an increasingly \\nunpredictable system from a climatic poin t of view. The planet will absorb these \\ngreenhouse gases without any problem, but the species that inhabit it will have enormous \\ndifficulties in adapting to new conditions. The scenario painted by the experts could not be \\nmore daunting, and urgent warnings for action  m u s t  b e  s e n t  o u t  t o  t h e  g e n e r a l  p u b l i c ,  \\nbusinesses and individuals.   \\nIn answer to this impending scenario, Carbonfeel  has been designed with a core mission: to \\norganize information and knowledge on the carb on footprint, making it universally useful \\nand accessible to all society. In short, the point is to pr ovide companies with the best \\navailable techniques for calculation and exchan ge of information within the processes of \\ninventory, management, reduction and offsetti ng of GHG emissions generated by their own \\nactivities. \\nThis information will allow companies to partic ipate actively in improving their behavior, \\nwithout having any effect on their business. Qu ite the contrary; their activities will start to \\nfocus on production patterns based on eco-effici ency and eco-design, and therefore lead to a \\nreduction in costs. Moreover, customers will recognize a continuous improvement effort \\nbased on a credible label supported by many different certifiers, consultants, companies,')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"minimum environmental impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key=\"\" # Optional if not set as an environment variable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "reteiver= db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":4})\n",
    "rqa=RetrievalQA.from_chain_type(llm=chat,\n",
    "                                chain_type=\"stuff\",\n",
    "                                retriever=reteiver,\n",
    "                                return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rqa.invoke(\"Carbonfeel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbonfeel is a collaborative initiative that provides a universal indicator, the carbon footprint (CF), to help organizations and individuals measure and reduce their environmental impact. It aims to facilitate the expansion of a responsible economy by promoting a credible label supported by various certifiers, consultants, companies, associations, universities, and others. Carbonfeel provides a procedural solution for calculating, verifying, certifying, and labeling the carbon footprint of organizations, products, and services. It seeks to make environmental accounting accessible to all businesses, regardless of size, and to promote transparency, comparability, and continuous improvement.\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_into_sections_and_chapters(text):\n",
    "    # Define regular expressions to identify sections and chapters\n",
    "    section_pattern = re.compile(r'Section \\d+\\s+.+')\n",
    "    chapter_pattern = re.compile(r'Chapter \\d+\\s+.+')\n",
    "\n",
    "    sections = section_pattern.split(text)\n",
    "    sections_titles = section_pattern.findall(text)\n",
    "\n",
    "    sections_with_chapters = []\n",
    "\n",
    "    for i, section in enumerate(sections):\n",
    "        if i == 0:\n",
    "            continue  # Skip the preface or any text before the first section\n",
    "\n",
    "        chapters = chapter_pattern.split(section)\n",
    "        chapter_titles = chapter_pattern.findall(section)\n",
    "        \n",
    "        chapters_with_content = []\n",
    "        for j in range(len(chapter_titles)):\n",
    "            if j + 1 < len(chapters):\n",
    "                chapters_with_content.append((chapter_titles[j], chapters[j + 1]))\n",
    "            else:\n",
    "                chapters_with_content.append((chapter_titles[j], chapters[j]))\n",
    "\n",
    "        sections_with_chapters.append((sections_titles[i - 1], chapters_with_content))\n",
    "    \n",
    "    return sections_with_chapters\n",
    "\n",
    "\n",
    "file_path = 'uploads\\globalwarming.txt'\n",
    "text = read_text_file(file_path)\n",
    "\n",
    "# Split the text into sections and chapters\n",
    "sections_with_chapters = split_into_sections_and_chapters(text)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sections_with_chapters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Encode sections and chapters\u001b[39;00m\n\u001b[0;32m     68\u001b[0m encoded_sections \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m section_title, chapters \u001b[38;5;129;01min\u001b[39;00m \u001b[43msections_with_chapters\u001b[49m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chapter_title, chapter_content \u001b[38;5;129;01min\u001b[39;00m chapters:\n\u001b[0;32m     71\u001b[0m         encoded_content \u001b[38;5;241m=\u001b[39m bi_encoder\u001b[38;5;241m.\u001b[39mencode(chapter_content, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sections_with_chapters' is not defined"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "bi_encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "\n",
    "# Function to save the uploaded PDF file\n",
    "def save_pdf(file):\n",
    "    with open(\"uploaded_file.pdf\", \"wb\") as f:\n",
    "        f.write(file.read())\n",
    "    return \"PDF saved successfully!\"\n",
    "\n",
    "\n",
    "# Function to display the PDF file\n",
    "def display_pdf(file):\n",
    "    return gr.File.update(value=file.name)\n",
    "\n",
    "\n",
    "def preprocess_pdf(file):\n",
    "    text = \"\"\n",
    "    with open(file, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "        sections_with_chapters = split_into_sections_and_chapters(text)  \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Encode sections and chapters\n",
    "        encoded_sections = []\n",
    "        for section_title, chapters in sections_with_chapters:\n",
    "            for chapter_title, chapter_content in chapters:\n",
    "                encoded_content = bi_encoder.encode(chapter_content, convert_to_tensor=True)\n",
    "                encoded_sections.append((section_title, chapter_title, chapter_content, encoded_content))\n",
    "    \n",
    "    with open(\"preprocessed_text.txt\", \"w\") as f:\n",
    "        f.write(text)\n",
    "    return \"PDF preprocessed successfully!\"\n",
    "\n",
    "\n",
    "# Function to handle chatbot queries using bi-encoder retrieval\n",
    "def chatbot_bi_encoder(query):\n",
    "    with open(\"preprocessed_text.txt\", \"r\") as f:\n",
    "        context = f.read()\n",
    "    context_embeddings = bi_encoder.encode(context, convert_to_tensor=True)\n",
    "    query_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    scores = util.pytorch_cos_sim(query_embedding, context_embeddings)[0]\n",
    "    best_idx = scores.argmax()\n",
    "    return context[best_idx:best_idx+200]\n",
    "\n",
    "\n",
    "def chatbot_qa(query):\n",
    "    with open(\"preprocessed_text.txt\", \"r\") as f:\n",
    "        context = f.read()\n",
    "    result = rqa.invoke(query)\n",
    "    return result['answer']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Encode sections and chapters\n",
    "encoded_sections = []\n",
    "for section_title, chapters in sections_with_chapters:\n",
    "    for chapter_title, chapter_content in chapters:\n",
    "        encoded_content = bi_encoder.encode(chapter_content, convert_to_tensor=True)\n",
    "        encoded_sections.append((section_title, chapter_title, chapter_content, encoded_content))\n",
    "\n",
    "\n",
    "\n",
    "def semantic_search(query, encoded_sections, model, k=10):\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Collect all embeddings and their associated titles and contents\n",
    "    all_embeddings = [item[3] for item in encoded_sections]\n",
    "    all_titles = [(item[0], item[1]) for item in encoded_sections]\n",
    "    all_contents = [item[2] for item in encoded_sections]\n",
    "    \n",
    "    # Perform semantic search\n",
    "    hits = util.semantic_search(query_embedding, all_embeddings, top_k=k)[0]\n",
    "    \n",
    "    # Collect results\n",
    "    results = [((all_titles[hit['corpus_id']][0], all_titles[hit['corpus_id']][1], all_contents[hit['corpus_id']]), hit['score']) for hit in hits]\n",
    "    return results\n",
    "\n",
    "# Perform a semantic search\n",
    "query = \"Carbonfeel\"\n",
    "results = semantic_search(query, encoded_sections, model, k=10)\n",
    "\n",
    "for (section_title, chapter_title, chapter_content), score in results:\n",
    "    print(f\"Section: {section_title}, Chapter: {chapter_title}, Score: {score}\\nContent: {chapter_content[:500]}...\\n\")  # Print first 500 characters for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, title, content=None, level=0):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.title = title\n",
    "        self.content = content if content else []\n",
    "        self.level = level\n",
    "        self.children = []\n",
    "        \n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TreeNode(id={self.id}, title={self.title}, level={self.level}, children={len(self.children)})\"\n",
    "\n",
    "def parse_book_content(book_content):\n",
    "    lines = book_content.split('\\n')\n",
    "    root = TreeNode(\"Textbook\")\n",
    "    current_section = None\n",
    "    current_chapter = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Section\") :\n",
    "            current_section = TreeNode(line, level=1)\n",
    "            root.add_child(current_section)\n",
    "            current_chapter = None\n",
    "            \n",
    "        elif line.startswith(\"Chapter\") :\n",
    "            current_chapter = TreeNode(line, level=2)\n",
    "            if current_section:\n",
    "                current_section.add_child(current_chapter)\n",
    "        elif current_chapter and line :\n",
    "            current_chapter.content.append(line)\n",
    "        \n",
    "          \n",
    "    \n",
    "    return root\n",
    "\n",
    "def print_tree(node, indent=0):\n",
    "    tree_str = \"  \" * indent + f\"{node.title} (ID: {node.id})\\n\"\n",
    "    for child in node.children:\n",
    "        tree_str += print_tree(child, indent + 1)\n",
    "        \n",
    "        \n",
    "    return tree_str\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = read_text_file(\"preprocessed_text.txt\")\n",
    "\n",
    "treenode = parse_book_content(text)\n",
    "\n",
    "output=print_tree(treenode)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_into_sections_and_chapters(text):\n",
    "    # Define regular expressions to identify sections and chapters\n",
    "    section_pattern = re.compile(r'Section \\d+\\s+.+')\n",
    "    chapter_pattern = re.compile(r'Chapter \\d+\\s+.+')\n",
    "\n",
    "    sections = section_pattern.split(text)\n",
    "    sections_titles = section_pattern.findall(text)\n",
    "\n",
    "    sections_with_chapters = []\n",
    "\n",
    "    for i, section in enumerate(sections):\n",
    "        if i == 0:\n",
    "            continue  # Skip the preface or any text before the first section\n",
    "\n",
    "        chapters = chapter_pattern.split(section)\n",
    "        chapter_titles = chapter_pattern.findall(section)\n",
    "        \n",
    "        chapters_with_content = []\n",
    "        for j in range(len(chapter_titles)):\n",
    "            if j + 1 < len(chapters):\n",
    "                chapters_with_content.append((chapter_titles[j], chapters[j + 1]))\n",
    "            else:\n",
    "                chapters_with_content.append((chapter_titles[j], chapters[j]))\n",
    "\n",
    "        sections_with_chapters.append((sections_titles[i - 1], chapters_with_content))\n",
    "    \n",
    "    return sections_with_chapters\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "file_path = 'uploads\\globalwarming.txt'\n",
    "text = read_text_file(file_path)\n",
    "\n",
    "# Split the text into sections and chapters\n",
    "sections_with_chapters = split_into_sections_and_chapters(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, encoded_sections, model, k=1):\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Collect all embeddings and their associated titles and contents\n",
    "    all_embeddings = [item[3] for item in encoded_sections]\n",
    "    all_titles = [(item[0], item[1]) for item in encoded_sections]\n",
    "    all_contents = [item[2] for item in encoded_sections]\n",
    "    \n",
    "    # Perform semantic search\n",
    "    hits = util.semantic_search(query_embedding, all_embeddings, top_k=k)[0]\n",
    "    \n",
    "    # Collect results\n",
    "    results = [((all_titles[hit['corpus_id']][0], all_titles[hit['corpus_id']][1], all_contents[hit['corpus_id']]), hit['score']) for hit in hits]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1897, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1483, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"f:\\projects\\RAG_HIndex\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 816, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\bhard\\AppData\\Local\\Temp\\ipykernel_22248\\2537863040.py\", line 11, in save_pdf\n",
      "    f.write(file.read())\n",
      "AttributeError: 'NamedString' object has no attribute 'read'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load models for the chatbot\n",
    "bi_encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to save the uploaded PDF file\n",
    "def save_pdf(file):\n",
    "    with open(\"uploaded_file.pdf\", \"wb\") as f:\n",
    "        f.write(file.read())\n",
    "    return \"PDF saved successfully!\"\n",
    "\n",
    "# Function to display the PDF file\n",
    "def display_pdf(file):\n",
    "    return gr.File.update(value=file.name)\n",
    "\n",
    "# Function to preprocess the PDF file\n",
    "def preprocess_pdf(file):\n",
    "    with open(file.name, 'rb') as f:  # Open in binary mode\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    with open(\"preprocessed_text.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    return \"PDF preprocessed successfully!\"\n",
    "\n",
    "# Function to handle chatbot queries using QA pipeline\n",
    "def chatbot_qa(query):\n",
    "    result = rqa.invoke(query)\n",
    "    return result['result']\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Function to handle chatbot queries using bi-encoder retrieval\n",
    "def chatbot_bi_encoder(query,k_top):\n",
    "    text = read_text_file(\"preprocessed_text.txt\")\n",
    "    sections_with_chapters = split_into_sections_and_chapters(text)    \n",
    "    \n",
    "    # Encode sections and chapters\n",
    "    encoded_sections = []\n",
    "    for section_title, chapters in sections_with_chapters:\n",
    "        for chapter_title, chapter_content in chapters:\n",
    "            encoded_content = bi_encoder.encode(chapter_content, convert_to_tensor=True)\n",
    "            encoded_sections.append((section_title, chapter_title, chapter_content, encoded_content))\n",
    "    \n",
    "    results = semantic_search(query, encoded_sections, bi_encoder, k_top)  \n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def display_tree():\n",
    "    text = read_text_file(\"preprocessed_text.txt\")\n",
    "    tree = parse_book_content(text)\n",
    "    return print_tree(tree)\n",
    "\n",
    "# Switchable chatbot function\n",
    "def chatbot(query, mode,slider):\n",
    "    if mode == \"QA_RAG\":\n",
    "        return chatbot_qa(query)\n",
    "    elif mode == \"Bi-Encoder\":\n",
    "        return chatbot_bi_encoder(query,slider)\n",
    "    elif mode == \"Heirarchial Structure\":\n",
    "        text = read_text_file(\"preprocessed_text.txt\")\n",
    "\n",
    "        treenode = parse_book_content(text)\n",
    "\n",
    "        output=print_tree(treenode)\n",
    "        return output\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h2 style=\"color: #FF5033; font-family: Arial, sans-serif; text-align: center;\">DOCSENSE</h2>\n",
    "    \"\"\")\n",
    "    \n",
    "    # PDF Upload and Display\n",
    "    pdf_file = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "    save_button = gr.Button(\"Save PDF\")\n",
    "    \n",
    "    save_button.click(save_pdf, inputs=pdf_file, outputs=None)\n",
    "    \n",
    "    # Preprocess PDF\n",
    "    preprocess_button = gr.Button(\"Preprocess PDF\")\n",
    "    preprocess_button.click(preprocess_pdf, inputs=pdf_file, outputs=None)\n",
    "    \n",
    "    # Chatbot\n",
    "    chatbot_input = gr.Textbox(label=\"Enter your query\")\n",
    "    mode_selector = gr.Radio([\"QA_RAG\", \"Bi-Encoder\",\"Heirarchial Structure\"], label=\"Select Chatbot Mode\", value=\"QA\")\n",
    "    slider_input = gr.Slider(label=\"K\",minimum=1, maximum=3,step=1)\n",
    "    chatbot_button = gr.Button(\"Get Response\")\n",
    "    \n",
    "    chatbot_output = gr.Textbox(label=\"Response\")\n",
    "    \n",
    "    \n",
    "    chatbot_button.click(chatbot, inputs=[chatbot_input, mode_selector,slider_input], outputs=chatbot_output)\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
